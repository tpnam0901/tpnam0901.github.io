---
---

@inproceedings{Springer:conf/EAI-INISCOM/3M-SER,
abbr      = {EAI INISCOM},
author    = {Tran, Phuong-Nam and Vu, Thuy-Duong Thi and Pham, Nhat Truong and Dang, Duc Ngoc Minh and Tran, Anh-Khoa},
editor    = {Vo, Nguyen-Son and Tran, Hoai-An},
title     = {Multi-modal Speech Emotion Recognition: Improving Accuracy Through Fusion of VGGish and BERT Features with Multi-head Attention},
booktitle = {Industrial Networks and Intelligent Systems},
year      = {2023},
publisher = {Springer Nature Switzerland},
address   = {Cham},
pages     = {148--158},
abstract  = {Recent research has shown that multi-modal learning is a successful method for enhancing classification performance by mixing several forms of input, notably in speech-emotion recognition (SER) tasks. However, the difference between the modalities may affect SER performance. To overcome this problem, a novel approach for multi-modal SER called 3M-SER is proposed in this paper. The 3M-SER leverages multi-head attention to fuse information from multiple feature embeddings, including audio and text features. The 3M-SER approach is based on the SERVER approach but includes an additional fusion module that improves the integration of text and audio features, leading to improved classification performance. To further enhance the correlation between the modalities, a LayerNorm is applied to audio features prior to fusion. Our approach achieved an unweighted accuracy (UA) and weighted accuracy (WA) of 79.96{\%} and 80.66{\%}, respectively, on the IEMOCAP benchmark dataset. This indicates that the proposed approach is better than SERVER and recent methods with similar approaches. In addition, it highlights the effectiveness of incorporating an extra fusion module in multi-modal learning.},
isbn      = {978-3-031-47359-3},
code      = {https://github.com/namphuongtran9196/3m-ser},
dimensions= {true},
html      = {https://link.springer.com/chapter/10.1007/978-3-031-47359-3_11},
doi       = {10.1007/978-3-031-47359-3_11},
preview   = {publication_preview/conference/eai_iniscom/3m-ser_icon.png}
}

@inproceedings{IEEE:conf/ICTC/ComSER,
  abbr      = {ICTC},
  author    = {Tran, Phuong-Nam and Vu, Thuy-Duong Thi and Pham, Nhat Truong and Dang-Ngoc, Hanh and Dang, Duc Ngoc Minh},
  title     = {Comparative analysis of multi-loss functions for enhanced multi-modal speech emotion recognition},
  booktitle = {2023 14th International Conference on Information and Communication Technology Convergence (ICTC)},
  publisher = {{IEEE}},
  year      = {2023},
  pages     = {425-429},
  code      = {https://github.com/namphuongtran9196/3m-ser},
  keywords={Emotion recognition;Analytical models;Speech recognition;Computer architecture;Speech enhancement;Natural language processing;Information and communication technology;center loss;contrastive-center loss;cross-entropy loss;multi-modal analysis;multi-modal model;speech emotion recognition},
  doi={10.1109/ICTC58733.2023.10392928},
  abstract  = {In recent years, multi-modal analysis has gained significant prominence across domains such as audio/speech processing, natural language processing, and affective computing, with a particular focus on speech emotion recognition (SER). The integration of data from diverse sources, encompassing text, audio, and images, in conjunction with classifier algorithms has led to the realization of enhanced performance in SER tasks. Traditionally, the cross-entropy loss function has been employed for the classification problem. However, it is challenging to discriminate the feature representations among classes for multi-modal classification tasks. In this study, we focus on the impact of the loss functions on multi-modal SER rather than designing the model architecture. Mainly, we evaluate the performance of multi-modal SER with different loss functions, such as cross-entropy loss, center loss, contrastive-center loss, and their combinations. Based on extensive comparative analysis, it is proven that the combination of cross-entropy loss and contrastive-center loss achieves the best performance for multi-modal SER. This combination reaches the highest accuracy of 80.27\% and the highest balanced accuracy of 81.44\% on the IEMOCAP dataset.},
  dimensions= {true},
  selected  = {true},
  preview   = {publication_preview/conference/ictc/ComSER_icon.png},
  html      = {https://ieeexplore.ieee.org/document/10392928},
}

@inproceedings{IEEE:conf/ICTC/RBBA,
  abbr      = {ICTC},
  author    = {Hoang, Duc-Hieu and Tran, Anh-Khoa and Dang, Duc Ngoc Minh and Tran, Phuong-Nam and Dang-Ngoc, Hanh and Nguyen, Cuong Tuan},
  title     = {RBBA: ResNet - BERT - Bahdanau Attention for Image Caption Generator},
  booktitle = {2023 14th International Conference on Information and Communication Technology Convergence (ICTC)},
  publisher = {{IEEE}},
  year      = {2023},
  pages     ={430-435},
  keywords  ={Training;Computer vision;Computational modeling;Generators;Natural language processing;Decoding;Information and communication technology;Deep learning;Natural language processing;Encoder-Decoder;Flickr8K;BLEU;Image Caption},
  doi       ={10.1109/ICTC58733.2023.10392496},
  abstract  = {In recent years, the topic of image caption generators has gained significant attention. Several successful projects have emerged in this field, showcasing notable advancements. Image caption generators automatically generate descriptive captions for images through the encoder and decoder mechanisms. The encoder leverages computer vision models, while the decoder utilizes natural language processing models. In this study, we aim to assess a comprehensive set of seven distinct methodologies, including six existing methods from prior research and one newly proposed. These methods are trained and evaluated with bilingual evaluation (BLEU) on the Flickr8K dataset. In our experiments, the proposed ResNet50 - BERT - Bahdanau Attention model outperforms the other models in terms of the BLEU-1 score of 0.532143 and BLEU-4 score of 0.126316.},
  dimensions= {true},
  html      = {https://ieeexplore.ieee.org/document/10392496/},
  preview   = {publication_preview/conference/ictc/RBBA_icon.png},

}

@inproceedings{IEEE:conf/ICTC/Vitexco,
  abbr      = {ICTC},
  author    = {Tran, Duong Thanh and Tran, Phuong-Nam and Nguyen, Doan Hieu Nguyen and Vu, Thuy-Duong Thi and Pham, Trung Thanh and Dang, Duc Ngoc Minh},

  title     = {Vitexco: Exemplar-based Video Colorization using Vision Transformer},
  booktitle = {2023 14th International Conference on Information and Communication Technology Convergence (ICTC)},
  publisher = {{IEEE}},
  year      = {2023},
  pages={59-64},
  keywords={Measurement;Visualization;Image color analysis;Transformers;Information and communication technology;Data mining;Kernel;image colorization;video colorization;exemplar-based;vision transformer},
  doi={10.1109/ICTC58733.2023.10393505},
  abstract  = {In the field of image and video colorization, the existing research employs a CNN to extract information from each video frame. However, due to the local nature of a kernel, it is challenging for CNN to capture the relationships between each pixel and others in an image, leading to inaccurate colorization. To solve this issue, we introduce an end-to-end network called Vitexco for colorizing videos. Vitexco utilizes the power of the Vision Transformer (ViT) to capture the relationships among all pixels in a frame with each other, providing a more effective method for colorizing video frames. We evaluate our approach on DAVIS datasets and demonstrate that it outperforms the state-of-the-art methods regarding color accuracy and visual quality. Our findings suggest that using a ViT can significantly enhance the performance of video colorization.},
  dimensions= {true},
  selected  = {true},
  preview   = {publication_preview/conference/ictc/Vitexco_icon.png},
  html      = {https://ieeexplore.ieee.org/document/10393505},
}
