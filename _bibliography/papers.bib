@article{ScientificReports,
  abbr       = {Scientific Reports},
  title      = {MemoCMT: multimodal emotion recognition using cross-modal transformer-based feature fusion},
  author     = {Khan, Mustaqeem and Tran, Phuong-Nam and Pham, Nhat Truong and El Saddik, Abdulmotaleb and Othmani, Alice},
  journal    = {Scientific reports},
  volume     = {15},
  number     = {1},
  pages      = {5473},
  year       = {2025},
  abstract   = {Speech emotion recognition has seen a surge in transformer models, which excel at understanding the overall message by analyzing long-term patterns in speech. However, these models come at a computational cost. In contrast, convolutional neural networks are faster but struggle with capturing these long-range relationships. Our proposed system, MemoCMT, tackles this challenge using a novel “cross-modal transformer” (CMT). This CMT can effectively analyze local and global speech features and their corresponding text. To boost efficiency, MemoCMT leverages recent advancements in pre-trained models: HuBERT extracts meaningful features from the audio, while BERT analyzes the text. The core innovation lies in how the CMT component utilizes and integrates these audio and text features. After this integration, different fusion techniques are applied before final emotion classification. Experiments show that MemoCMT achieves impressive performance, with the CMT using min aggregation achieving the highest unweighted accuracy (UW-Acc) of 81.33% and 91.93%, and weighted accuracy (W-Acc) of 81.85% and 91.84% respectively on benchmark IEMOCAP and ESD corpora. The results of our system demonstrate the generalization capacity and robustness for real-world industrial applications. Moreover, the implementation details of MemoCMT are publicly available at https://github.com/tpnam0901/MemoCMT/ for reproducibility purposes.},
  dimensions = {true},
  selected   = {true},
  publisher  = {Nature Publishing Group UK London},
  doi        = {10.1038/s41598-025-89202-x},
  html       = {https://www.nature.com/articles/s41598-025-89202-x},
  preview    = {publication_preview/nature_sr.png},
  code       = {https://github.com/tpnam0901/MemoCMT}
}

@article{quiaoyu2025deepseek,
  abbr       = {ACM Comput. Surv.},
  author     = {Qiao, Yu and Tran, Phuong-Nam and Yoon, Ji and Nguyen, Loc and Huh, Eui-Nam and Niyato, Dusit (Tao) and Hong, Choong Seon},
  title      = {DeepSeek-Inspired Exploration of RL-Based LLMs and Synergy with Wireless Networks: A Survey},
  year       = {2025},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  issn       = {0360-0300},
  html       = {https://doi.org/10.1145/3776745},
  doi        = {10.1145/3776745},
  abstract   = {Recently, reinforcement learning (RL)-based large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, have attracted significant attention for their impressive capabilities in multimodal data understanding. Meanwhile, as information services rapidly expand, there is increasing demand for intelligent and adaptable wireless networks to support this growth. Integrating LLMs with wireless networks offers a promising solution, as LLMs enhance network optimization with strong reasoning and decision-making abilities, while wireless infrastructure enables broad deployment of these models. This synergy calls for deeper exploration into the convergence of these two fields. To this end, this paper makes a timely contribution by first reviewing key technologies for wireless network optimization, establishing a foundation for understanding how LLMs can be effectively integrated. We then turn to recent advancements in RL-based LLMs, particularly the open-source DeepSeek models, as they offer greater accessibility and customizability for researchers and practitioners. Subsequently, we explore the synergy between these fields, emphasizing motivations, open challenges, and potential solutions. Finally, we provide insights into future directions and societal impacts. Overall, this survey offers a comprehensive exploration of the relationship between LLMs and wireless networks, presenting a vision of how these domains empower each other to drive innovation.},
  journal    = {ACM Comput. Surv.},
  dimensions = {true},
  selected   = {true},
  keywords   = {Reinforcement learning, LLMs, RL-based LLMs, wireless networks},
  preview    = {publication_preview/acmcs.png}
}