@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Deep Learning-Based Automated Cashier System for Bakeries},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Innovative Multi-Modal Control for Surveillance Spider Robots:
Voice and Hand Gesture Integration},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{IEEE:conf/ICIIT/,
  abbr      = {},
  author    = {},
  title     = {Vietnamese Traffic Sign Recognition Using Deep Learning},
  booktitle = {},
  publisher = {},
  year      = {},
  code      = {},
  abstract  = {},
  dimensions= {true},
  selected  = {},
  preview   = {},
}

@inproceedings{Springer:conf/EAI-INISCOM/PerfTSR,
abbr      = {EAI INISCOM},
author    = {Nguyen, Huu Duc and Thanh, Nam Vo and Tran, Phuong-Nam and Nguyen, Cuong Tuan and Dang, Duc Ngoc Minh },
editor    = {},
title     = {Performance Comparison in Traffic Sign Recognition using Deep Learning},
booktitle = {},
year      = {2024},
publisher = {},
address   = {},
pages     = {},
abstract  = {In recent years, along with the increase in private cars, traffic signs have increased in quantity, demanding greater attentiveness from drivers. Many studies have been conducted on Traffic Sign Recognition (TSR) to enhance road safety and driver assistance systems by enabling vehicles to autonomously detect and interpret traffic signs, providing crucial information to drivers in real-time. This paper examines various traffic sign detection and classification models, which give practitioners and researchers valuable insights into selecting optimal solutions tailored to the needs of real-world applications. Notably, the investigation highlights YOLOv8 as a leading detection model, displaying exceptional results with an mAP of 99.4\%. YOLOv8 provides various model sizes allowing for adaptation to specific real-time processing scenarios. On the other hand, the LeNet model is a standout performer in the classification domain, consistently achieving a remarkable accuracy of 98.2\% while using only 0.4 million parameters. The LeNet architecture ensures accurate and rapid traffic sign classification, making it an appealing choice for applications where resource efficiency is critical.},
isbn      = {},
code      = {},
selected  = {},
dimensions= {true},
html      = {},
doi       = {},
}


@inproceedings{IEEE:conf/ICOIN/MERSA,
  abbr      = {ICOIN},
  author    = {Bao Le, Quan  and Tuan Trinh, Kiet and Pham, Nhat Truong and Nguyen, Dinh Hung Son and Tran, Phuong-Nam and Tuan Nguyen, Cuong  and Dang, Duc Ngoc Minh},
  title     = {MERSA: Multimodal Emotion Recognition with Self-Align Embedding},
  booktitle = {},
  publisher = {},
  year      = {2024},
  code      = {},
  abstract  = {Emotions are an integral part of human communication and interaction, significantly shaping our social connections, decision-making, and overall well-being. Understanding and analyzing emotions have become essential in various fields, including psychology, human-computer interaction, marketing, and healthcare.  The previous approach has indeed made significant strides in improving the accuracy of predicting emotions within speech. However, the current model's performance still falls short when it comes to real-life applications. This limitation arises due to several factors such as lack of context, ambiguity in speech and meaning, and other contributing elements. To reduce the ambiguity of emotions within speech, this paper seeks to leverage multiple data modalities, specifically textual and acoustic information. To analyze these modalities, we propose a novel approach called MERSA which utilizes the self-align method to extract context features from both textual and acoustic information. By leveraging this technique, the MERSA model can effectively create fusion feature vectors of the multiple inputs, facilitating a more accurate and holistic analysis of emotions within speech. Moreover, the MERSA model has incorporated a cross-attention module into its network architecture, which enables the MERSA model to capture and leverage the interdependencies between the textual and acoustic modalities.},
  dimensions= {true},
  selected  = {true},
  preview   = {icoin/mersa.png},
}

